{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph Vector Model for Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'shared_lib.utils' from 'shared_lib/utils.py'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "from random import shuffle\n",
    "import progressbar\n",
    "from collections import namedtuple\n",
    "\n",
    "# Gensim library\n",
    "import gensim\n",
    "from gensim.models import *\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "# Import SKLearn Tools\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import utils\n",
    "from shared_lib import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Input Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingDate</th>\n",
       "      <th>reviewComments</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>modelId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>08/15/2017</td>\n",
       "      <td>beautifullllllllllllllllllllllllllllllllllllll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4002178_W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>05/08/2017</td>\n",
       "      <td>The actual product came out looking much diffe...</td>\n",
       "      <td>Disappointing</td>\n",
       "      <td>4002178_W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>04/10/2017</td>\n",
       "      <td>These shoes look nothing like the picture! I e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4002178_W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>02/26/2017</td>\n",
       "      <td>I ordered this shoe because i loved the displa...</td>\n",
       "      <td>color sample was way off</td>\n",
       "      <td>4002178_W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>09/15/2017</td>\n",
       "      <td>They are comfortable sneakers for working out ...</td>\n",
       "      <td>awesome sneakers</td>\n",
       "      <td>4002179_W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  ratingDate                                     reviewComments  \\\n",
       "0       4  08/15/2017  beautifullllllllllllllllllllllllllllllllllllll...   \n",
       "1       1  05/08/2017  The actual product came out looking much diffe...   \n",
       "2       1  04/10/2017  These shoes look nothing like the picture! I e...   \n",
       "3       1  02/26/2017  I ordered this shoe because i loved the displa...   \n",
       "4       5  09/15/2017  They are comfortable sneakers for working out ...   \n",
       "\n",
       "                reviewTitle    modelId  \n",
       "0                       NaN  4002178_W  \n",
       "1             Disappointing  4002178_W  \n",
       "2                       NaN  4002178_W  \n",
       "3  color sample was way off  4002178_W  \n",
       "4          awesome sneakers  4002179_W  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewData_df = pd.read_csv(\"product_reviews.csv\")\n",
    "reviewData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingDate</th>\n",
       "      <th>reviewComments</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>modelId</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>08/15/2017</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4002178_W</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>05/08/2017</td>\n",
       "      <td>actual product came looking much different onl...</td>\n",
       "      <td>Disappointing</td>\n",
       "      <td>4002178_W</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>04/10/2017</td>\n",
       "      <td>shoes look nothing like picture expected grey ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4002178_W</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>02/26/2017</td>\n",
       "      <td>ordered shoe loved displayed blush pink color ...</td>\n",
       "      <td>color sample was way off</td>\n",
       "      <td>4002178_W</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>09/15/2017</td>\n",
       "      <td>comfortable sneakers working running</td>\n",
       "      <td>awesome sneakers</td>\n",
       "      <td>4002179_W</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  ratingDate                                     reviewComments  \\\n",
       "0       4  08/15/2017                                          beautiful   \n",
       "1       1  05/08/2017  actual product came looking much different onl...   \n",
       "2       1  04/10/2017  shoes look nothing like picture expected grey ...   \n",
       "3       1  02/26/2017  ordered shoe loved displayed blush pink color ...   \n",
       "4       5  09/15/2017               comfortable sneakers working running   \n",
       "\n",
       "                reviewTitle    modelId  label  \n",
       "0                       NaN  4002178_W      0  \n",
       "1             Disappointing  4002178_W      2  \n",
       "2                       NaN  4002178_W      2  \n",
       "3  color sample was way off  4002178_W      2  \n",
       "4          awesome sneakers  4002179_W      0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewData_df['reviewComments'] = reviewData_df['reviewComments'].apply(utils.preprocess_doc)\n",
    "reviewData_df['label'] = reviewData_df.apply(utils.create_senti_label_num, axis = 1)\n",
    "reviewData_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## paragraph vector model\n",
    "\n",
    "### labeled sentence data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to convert newsgroup corpus into paragraph2Vec formats\n",
    "def convert_reviews(docs, split):\n",
    "    #global doc_count\n",
    "    tagged_documents = []\n",
    "    \n",
    "    for i,v in enumerate(docs):\n",
    "        label = '%s_%s'%(split, i)\n",
    "        tagged_documents.append(LabeledSentence(v, [label]))\n",
    "    \n",
    "    return tagged_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train (80%), validation (20%), and test split (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4288 docs (434214 tokens)\n",
      "Training set: 3430 docs (352393 tokens)\n",
      "Test set: 858 docs (81821 tokens)\n",
      "Loaded 3430 docs (352393 tokens)\n",
      "Training set: 2572 docs (262842 tokens)\n",
      "Validation set: 858 docs (89551 tokens)\n"
     ]
    }
   ],
   "source": [
    "all_reviews = reviewData_df.loc[:, 'reviewComments']\n",
    "all_labels = reviewData_df.loc[:, 'label']\n",
    "\n",
    "# Split for test set\n",
    "train_docs, train_labels, test_docs, test_labels = utils.get_train_test_docs(all_reviews, \n",
    "                                                                             all_labels, \n",
    "                                                                             split = 0.8, \n",
    "                                                                             shuffle = True)\n",
    "\n",
    "# Further split for validation\n",
    "train_docs, train_labels, validation_docs, validation_labels = utils.get_train_val_docs(train_docs, \n",
    "                                                                                        train_labels, \n",
    "                                                                                        split = 0.75, \n",
    "                                                                                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4288 docs: 2572 train, 858 validation, 858 test\n"
     ]
    }
   ],
   "source": [
    "## Convert to paragraph2vec library input format\n",
    "train_docs = convert_reviews(train_docs,'train')\n",
    "validation_docs = convert_reviews(validation_docs,'validation')\n",
    "test_docs = convert_reviews(test_docs, 'test')\n",
    "\n",
    "## We combine all data to build the paragraph vector model\n",
    "all_docs = []\n",
    "all_docs.extend(train_docs)\n",
    "all_docs.extend(validation_docs)\n",
    "all_docs.extend(test_docs)\n",
    "doc_list = all_docs[:]  # for reshuffling per pass\n",
    "\n",
    "print '{} docs: {} train, {} validation, {} test'.format(len(doc_list), len(train_docs), \\\n",
    "                                                          len(validation_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paragraph Vector model\n",
    "\n",
    "* Use feature size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = 0 use distributed bag of words, dm=1 distributed memory\n",
    "dm_model = Doc2Vec(dm=1, dm_mean=1, sample=1e-5, size=100, window=10, \n",
    "                   negative=5, hs=0, min_count=2, workers=10, max_vocab_size = 20000)\n",
    "dm_model.build_vocab(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 100)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "bar = progressbar.ProgressBar()\n",
    "\n",
    "for epoch in bar(range(passes)):\n",
    "    shuffle(doc_list)\n",
    "    dm_model.alpha, dm_model.min_alpha = alpha, alpha\n",
    "    dm_model.train(doc_list, total_examples=len(doc_list))\n",
    "    alpha -= alpha_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features vectors from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Vectors From Doc2Vec\n",
    "def extract_vectors(model, docs):    \n",
    "    vectors_list = []\n",
    "    for doc_no in range(len(docs)):\n",
    "        doc_label = docs[doc_no].tags[0] # Use tag to id\n",
    "        doc_vector = model.docvecs[doc_label]\n",
    "        vectors_list.append(doc_vector)      \n",
    "    return vectors_list\n",
    "\n",
    "# TODO inferred vectors\n",
    "def get_infer_vectors(model,docs):   \n",
    "    vecs = []\n",
    "    for doc in docs:\n",
    "        vecs.append(model.infer_vector(doc.words))\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training doc shape: (2572, 100)\n",
      "Validation doc shape: (858, 100)\n",
      "Test doc shape: (858, 100)\n",
      "Training label shape: (2572, 3)\n",
      "Validation label shape: (858, 3)\n",
      "Test label shape: (858, 3)\n"
     ]
    }
   ],
   "source": [
    "train_docs_ids = np.array(extract_vectors(dm_model,train_docs))\n",
    "validation_docs_ids = np.array(extract_vectors(dm_model,validation_docs))\n",
    "test_docs_ids = np.array(extract_vectors(dm_model,test_docs))\n",
    "\n",
    "# Convert label to one-hot-code\n",
    "num_class = 3 # pos, neg, neu\n",
    "train_labels_oh = np.eye(num_class)[train_labels]\n",
    "validation_labels_oh = np.eye(num_class)[validation_labels]\n",
    "test_labels_oh = np.eye(num_class)[test_labels]\n",
    "\n",
    "print \"Training doc shape: {}\".format(train_docs_ids.shape)\n",
    "print \"Validation doc shape: {}\".format(validation_docs_ids.shape)\n",
    "print \"Test doc shape: {}\".format(test_docs_ids.shape)\n",
    "print \"Training label shape: {}\".format(train_labels_oh.shape)\n",
    "print \"Validation label shape: {}\".format(validation_labels_oh.shape)\n",
    "print \"Test label shape: {}\".format(test_labels_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Tensorflow softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = num_class\n",
    "feature_size = 100\n",
    "l2_reg_lambda = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ is the Paragraph2vec input vectors\n",
    "x_ = tf.placeholder(tf.float32, [None, feature_size], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name=\"y\")\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "# Output Layer: Softmax (1 affine layer)\n",
    "with tf.name_scope(\"Output_layer\"):\n",
    "    Z_ = tf.Variable(tf.random_uniform([feature_size, num_classes], -1.0, 1.0), name = \"Z\")\n",
    "    b_ = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "    logits_ = tf.add(tf.matmul(x_, Z_), b_, name=\"logits\")\n",
    "    \n",
    "    # L2 Regularization\n",
    "    l2_loss += tf.nn.l2_loss(Z_)\n",
    "    l2_loss += tf.nn.l2_loss(b_)\n",
    "    predictions_ = tf.argmax(logits_, 1, name=\"predictions\")\n",
    "\n",
    "    \n",
    "# Calculate mean cross-entropy loss\n",
    "with tf.name_scope(\"cost_function\"):\n",
    "    per_example_losses_ = tf.nn.softmax_cross_entropy_with_logits(logits=logits_, \n",
    "                                                                 labels=y_,\n",
    "                                                                 name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_losses_) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions_ = tf.equal(predictions_, tf.argmax(y_, 1))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_predictions_, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"Training\"):\n",
    "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
    "    #optimizer_ = tf.train.AdamOptimizer(alpha_)\n",
    "    train_step_ = optimizer_.minimize(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\"\"\"\n",
    "    for i in xrange(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "        \n",
    "def train_batch(session, batch, alpha):\n",
    "    # Feed last column as targets\n",
    "    feed_dict = {x_:train_docs_ids,\n",
    "                 y_:train_labels_oh,\n",
    "                 alpha_:alpha}\n",
    "    c, a, pred, _ = session.run([loss_, accuracy_, predictions_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c, a, pred        \n",
    "\n",
    "def validation_batch(session):\n",
    "    feed_dict = {x_:validation_docs_ids,\n",
    "                 y_:validation_labels_oh}\n",
    "    a, pred = session.run([accuracy_, predictions_, ], feed_dict=feed_dict)\n",
    "    return a, pred\n",
    "\n",
    "def predict_batch(session):\n",
    "    feed_dict = {x_:test_docs_ids,\n",
    "                 y_:test_labels_oh}\n",
    "    a, pred = session.run([accuracy_, predictions_], feed_dict=feed_dict)\n",
    "    return a, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(num_epochs, learning_rate, batch_size = 100, min_rate = 0.1, print_freq = 10, seed = 42):\n",
    "    # One epoch = one pass through the training data\n",
    "    num_epochs = num_epochs\n",
    "    batch_size = batch_size\n",
    "    alpha = learning_rate  # learning rate\n",
    "    min_alpha = min_rate\n",
    "    alpha_delta = (alpha - min_alpha) / num_epochs\n",
    "    print_every = print_freq\n",
    "\n",
    "    # Initializer step\n",
    "    init_ = tf.global_variables_initializer()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # For plotting\n",
    "    train_accuracy = []\n",
    "    validation_accuracy = []\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(init_)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        epoch_cost = 0.0\n",
    "        total_batches = 0\n",
    "        print \"\"\n",
    "        print \"----------- Training ------------\"\n",
    "        for i, batch in enumerate(batch_generator(train_docs_ids, batch_size)):\n",
    "            if (i % print_every == 0):\n",
    "                print \"[epoch %d] seen %d minibatches\" % (epoch, i)\n",
    "\n",
    "            cost, accuracy, pred = train_batch(session, batch, alpha)\n",
    "            epoch_cost += cost\n",
    "            total_batches = i + 1\n",
    "\n",
    "        avg_cost = epoch_cost / total_batches\n",
    "        alpha = alpha - alpha_delta\n",
    "\n",
    "        print \"[epoch %d] Completed %d minibatches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch))\n",
    "        print \"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,)\n",
    "        print \"[epoch %d] Accuracy %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Training Classificaiton Report\\n\" %(epoch)\n",
    "        print pred.shape\n",
    "        print classification_report(train_labels, pred)\n",
    "        print \"Training Vector Accuracy: \", accuracy_score(train_labels, pred)\n",
    "        train_accuracy.append(accuracy)\n",
    "        \n",
    "        print \"\"\n",
    "        print \"---------- Validation ----------\"\n",
    "        accuracy, pred = validation_batch(session)\n",
    "        print \"[epoch %d] Validation Accuracy is %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Validation Classificaiton Report\\n\" %(epoch)\n",
    "        print classification_report(validation_labels, pred)\n",
    "        print \"Validation Vector Accuracy:\", accuracy_score(validation_labels, pred)\n",
    "        validation_accuracy.append(accuracy)        \n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(session, \"./saved_model/p2v_model\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    return train_accuracy, validation_accuracy, session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] Completed 25 minibatches in 0:00:00\n",
      "[epoch 1] Average cost: 0.478\n",
      "[epoch 1] Accuracy 0.885\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 1] Validation Accuracy is 0.871\n",
      "[epoch 1] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] Completed 25 minibatches in 0:00:00\n",
      "[epoch 2] Average cost: 0.432\n",
      "[epoch 2] Accuracy 0.885\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 2] Validation Accuracy is 0.871\n",
      "[epoch 2] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] Completed 25 minibatches in 0:00:00\n",
      "[epoch 3] Average cost: 0.432\n",
      "[epoch 3] Accuracy 0.885\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 3] Validation Accuracy is 0.871\n",
      "[epoch 3] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] Completed 25 minibatches in 0:00:00\n",
      "[epoch 4] Average cost: 0.431\n",
      "[epoch 4] Accuracy 0.885\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 4] Validation Accuracy is 0.871\n",
      "[epoch 4] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] Completed 25 minibatches in 0:00:00\n",
      "[epoch 5] Average cost: 0.431\n",
      "[epoch 5] Accuracy 0.885\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 5] Validation Accuracy is 0.871\n",
      "[epoch 5] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 6] seen 0 minibatches\n",
      "[epoch 6] Completed 25 minibatches in 0:00:00\n",
      "[epoch 6] Average cost: 0.431\n",
      "[epoch 6] Accuracy 0.885\n",
      "[epoch 6] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 6] Validation Accuracy is 0.871\n",
      "[epoch 6] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 7] seen 0 minibatches\n",
      "[epoch 7] Completed 25 minibatches in 0:00:00\n",
      "[epoch 7] Average cost: 0.431\n",
      "[epoch 7] Accuracy 0.885\n",
      "[epoch 7] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 7] Validation Accuracy is 0.871\n",
      "[epoch 7] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 8] seen 0 minibatches\n",
      "[epoch 8] Completed 25 minibatches in 0:00:00\n",
      "[epoch 8] Average cost: 0.430\n",
      "[epoch 8] Accuracy 0.885\n",
      "[epoch 8] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 8] Validation Accuracy is 0.871\n",
      "[epoch 8] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 9] seen 0 minibatches\n",
      "[epoch 9] Completed 25 minibatches in 0:00:00\n",
      "[epoch 9] Average cost: 0.430\n",
      "[epoch 9] Accuracy 0.885\n",
      "[epoch 9] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 9] Validation Accuracy is 0.871\n",
      "[epoch 9] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 10] seen 0 minibatches\n",
      "[epoch 10] Completed 25 minibatches in 0:00:00\n",
      "[epoch 10] Average cost: 0.430\n",
      "[epoch 10] Accuracy 0.885\n",
      "[epoch 10] Training Classificaiton Report\n",
      "\n",
      "(2572,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      2275\n",
      "          1       0.00      0.00      0.00       211\n",
      "          2       0.00      0.00      0.00        86\n",
      "\n",
      "avg / total       0.78      0.88      0.83      2572\n",
      "\n",
      "Training Vector Accuracy:  0.884525660964\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 10] Validation Accuracy is 0.871\n",
      "[epoch 10] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       747\n",
      "          1       0.00      0.00      0.00        78\n",
      "          2       0.00      0.00      0.00        33\n",
      "\n",
      "avg / total       0.76      0.87      0.81       858\n",
      "\n",
      "Validation Vector Accuracy: 0.870629370629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ./saved_model/p2v_model\n"
     ]
    }
   ],
   "source": [
    "train_accur, validation_accur, sess = run_epochs(num_epochs = 10, \n",
    "                                                 batch_size = 100, \n",
    "                                                 learning_rate = 0.5, \n",
    "                                                 min_rate = 0.1, \n",
    "                                                 print_freq = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1-vs-Rest SVM ---\n",
      "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianyu/anaconda/envs/tensorflow/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 336 out of 336 | elapsed:   58.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.310280373832\n",
      "{'estimator__kernel': 'poly', 'estimator__C': 0.001, 'estimator__degree': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = OneVsRestClassifier(SVC(probability=True))\n",
    "clf_svc.fit(train_docs_ids, train_labels)\n",
    "\n",
    "parameters_SVC = {\n",
    "    \"estimator__C\": [0.001,0.01,0.1,0.5,1,2,5],\n",
    "    \"estimator__kernel\": [\"poly\",\"rbf\", \"sigmoid\",\"linear\"],\n",
    "    \"estimator__degree\": [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "print \"--- 1-vs-Rest SVM ---\"\n",
    "mod_svc = GridSearchCV(estimator=clf_svc, param_grid=parameters_SVC, scoring='f1_macro', verbose=True)\n",
    "mod_svc.fit(validation_docs_ids, validation_labels)\n",
    "print mod_svc.best_score_ \n",
    "print mod_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = OneVsRestClassifier(SVC(probability=True, \n",
    "                              kernel=mod_svc.best_params_['estimator__kernel'], \n",
    "                              C=mod_svc.best_params_['estimator__C'], \n",
    "                              degree=mod_svc.best_params_['estimator__degree']))\n",
    "\n",
    "clf_svc.fit(train_docs_ids, train_labels)\n",
    "pred = clf_svc.predict(test_docs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94       755\n",
      "          1       0.00      0.00      0.00        76\n",
      "          2       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.77      0.88      0.82       858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(test_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
